[
  {
    "objectID": "decision_tree.html",
    "href": "decision_tree.html",
    "title": "",
    "section": "",
    "text": "In this tab, a classification method called Decision Tree is used. This method continuouslly asks “whether or not questions” to the input so that it can gradually divides it into different parts.  \n\n(Example of the process of decision tree) reference\n\n But how to decide what to ask? The trick here is to use mathematical formulas to quantify one of the following metrics to find the question.\n\n\nThe extent to which we gain new information from new answers (by calculating entropy)\nThe probability which we incorrectly classify the sample (by calculating gini index)\n\n\nBy using this method, we can easily find how important an attribute is, and understand what makes our research target so different.\nNow we are going to apply this method to our data to achieve one of our goals:\n\n\nDistinguish users who are more likely to be influenced by rumor with other users."
  },
  {
    "objectID": "decision_tree.html#class-distribution",
    "href": "decision_tree.html#class-distribution",
    "title": "",
    "section": "Class Distribution",
    "text": "Class Distribution\n\n\nCode\n# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\n#load the data\ndf=pd.read_csv(\"../../data/01-modified-data/cleaned_followers.csv\")\nsns.set_theme()\n#plot the distribution\nplt.hist(df.label.astype(\"string\"))\nplt.title(\"The distribution of the class\",fontsize=18)\nplt.xlabel(\"Class\",fontsize=16)\nplt.ylabel(\"Counts\",fontsize=16)\n#show the data\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      user_id\n      screen_name\n      followers_count\n      friends_count\n      listed_count\n      favourites_count\n      tweet_num\n      protected\n      verfied\n      label\n    \n  \n  \n    \n      0\n      2198516225\n      _Banzi_\n      27\n      199\n      0\n      601\n      122\n      0\n      0\n      1\n    \n    \n      1\n      1504258025804210176\n      DelorbeTori\n      81\n      2771\n      0\n      0\n      6\n      0\n      0\n      1\n    \n    \n      2\n      1581474148571877377\n      omkarVyawahare2\n      0\n      56\n      0\n      24\n      0\n      0\n      0\n      1\n    \n    \n      3\n      2967681610\n      L1BERTE_S\n      50\n      68\n      1\n      8735\n      2238\n      0\n      0\n      1\n    \n    \n      4\n      1400890434\n      mistamomo_\n      356\n      1104\n      19\n      18347\n      20160\n      0\n      0\n      1\n    \n  \n\n\n\n\n\n\n\nThe count of two classes are the same, 350 for each. This is dsigned when gathering the data(“details of followers”). With this kind of data, we can avoid problems brought from imbalanced data sets."
  },
  {
    "objectID": "decision_tree.html#baseline-model-for-comparsion",
    "href": "decision_tree.html#baseline-model-for-comparsion",
    "title": "",
    "section": "Baseline Model for Comparsion",
    "text": "Baseline Model for Comparsion\n\n\nCode\n#define a baseline model which random assign labels\ndef random_classifier(y_data):\n    ypred=[]\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data,ypred))\n\nrandom_classifier(df.label)\n\n\n-----RANDOM CLASSIFIER-----\naccuracy 0.5114285714285715\npercision, recall, fscore, (array([0.51212121, 0.51081081]), array([0.48285714, 0.54      ]), array([0.49705882, 0.525     ]), array([350, 350], dtype=int64))\n\n\n What a baseline model here did is guess the class. And we can see that every metric is around 50%. So if a model perform than this baseline, than we can say it do make some sense."
  },
  {
    "objectID": "decision_tree.html#feature-selection",
    "href": "decision_tree.html#feature-selection",
    "title": "",
    "section": "Feature Selection",
    "text": "Feature Selection\n\n\nCode\n# id reflect the time account exits,we normalized it \ndf[\"user_id\"]=(df.user_id-df.user_id.mean())/df.user_id.std() \n#drop the feature we won't consider about\ndf.drop(columns=[\"screen_name\"],axis=1,inplace=True)\n#\nX=df.drop(columns=[\"label\"],axis=1)\ny=df[\"label\"]\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\n\n\n\n\n\n\n\n\nFeature\nMeaning\n\n\n\n\nuser_id\nthe id of users\n\n\nfollowers_count\nthe number of followers this account currently has\n\n\nfriends_count\nthe number of users this account is following\n\n\nlisted_count\nthe number of public lists that this user is a member of\n\n\nfavourites_count\nthe number of Tweets this user has liked in the account’s lifetime\n\n\ntweet_num\nthe number of Tweets (including retweets) issued by the user\n\n\nprotected\nwhether user has chosen to protect their Tweets\n\n\nverified\nwhether user has a verified account\n\n\n\n\n(The names and meanings of features)\n\n8 features was selected to train the model. These features are all attributes that are allowed to get and reflect the character of accounts. With these attributes, we have the biggest possibility to find the differences. Note that “user_id” is also selected because it reflect how long an account was created. The bigger an user id is, the newer the account is."
  },
  {
    "objectID": "decision_tree.html#model-tuning",
    "href": "decision_tree.html#model-tuning",
    "title": "",
    "section": "Model tuning",
    "text": "Model tuning\n\n\nCode\n# try different numbers of layers to find the best one\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train,y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0),recall_score(y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label=0),recall_score(y_train, yp_train,pos_label=1)])\ntest_results=np.array(test_results)\ntrain_results=np.array(train_results)\n#generate plots of the performance of different layers \ndef metric_plot(ylabel,layer,yptrain,yptest):\n    fig=plt.figure()\n    plt.plot(layer,yptrain,'o-',color=\"b\")\n    plt.plot(layer,yptest,'o-',color=\"r\")\n    plt.ylabel(ylabel+\" Training (blue) and Test (red)\",fontsize=16)\n    plt.xlabel(\"Number of layers in decision tree(max_depth)\",fontsize=16)\nmetric_plot(\"ACCURACY(Y=0)\",test_results[:,0],train_results[:,1],test_results[:,1])\nmetric_plot(\"RECALL(Y=0)\",test_results[:,0],train_results[:,2],test_results[:,2])\nmetric_plot(\"RECALL(Y=1)\",test_results[:,0],train_results[:,3],test_results[:,3])\n\n\n\n\n\n\n\n\n\n\n\n To find the most suitable number of layers, several plots was produced. We finally find that we should set max_depth as 4."
  },
  {
    "objectID": "decision_tree.html#final-results",
    "href": "decision_tree.html#final-results",
    "title": "",
    "section": "Final Results",
    "text": "Final Results\n\n\nCode\n#fit the tree model with the best layer\nmodel = tree.DecisionTreeClassifier(max_depth=4)\nmodel = model.fit(x_train,y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n#write a function to visualize the confusion matrix\ndef confusion_plot(y_data,y_pred):\n    print(\n        \"ACCURACY: \"+str(accuracy_score(y_data,y_pred))+\"\\n\"+\n        \"NEGATIVE RECALL (Y=0): \"+str(recall_score(y_data,y_pred,pos_label=0))+\"\\n\"+\n        \"NEGATIVE PRECISION (Y=0): \"+str(precision_score(y_data,y_pred,pos_label=0))+\"\\n\"+\n        \"POSITIVE RECALL (Y=1): \"+str(recall_score(y_data,y_pred,pos_label=1))+\"\\n\"+\n        \"POSITIVE PRECISION (Y=1): \"+str(precision_score(y_data,y_pred,pos_label=1))+\"\\n\"\n    )\n    cf=confusion_matrix(y_data, y_pred)\n    # customize the anno\n    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n    group_counts = [\"{0:0.0f}\".format(value) for value in cf.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    #plot the heatmap\n    fig=sns.heatmap(cf, annot=labels, fmt=\"\", cmap='Blues')\n    plt.title(\"Confusion Matrix of Texts - Decision Tree\",fontsize=18)\n    fig.set_xticklabels([\"Easily affected\",\"Not easily affected\"],fontsize=13)\n    fig.set_yticklabels([\"Easily affected\",\"Not easily affected\"],fontsize=13)\n    fig.set_xlabel(\"Predicted Labels\",fontsize=14)\n    fig.set_ylabel(\"True Labels\",fontsize=14)\n    plt.show()\nconfusion_plot(y_test,yp_test)\n\n#write a function to visualize the tree\ndef plot_tree(model,X,Y):\n    fig = plt.figure(figsize=(10,8))\n    tree_vis= tree.plot_tree(model, feature_names=X.columns,class_names=Y.name,filled=True)\nplot_tree(model,x_test,y_test)\n\n\nACCURACY: 0.6714285714285714\nNEGATIVE RECALL (Y=0): 0.5064935064935064\nNEGATIVE PRECISION (Y=0): 0.8297872340425532\nPOSITIVE RECALL (Y=1): 0.873015873015873\nPOSITIVE PRECISION (Y=1): 0.5913978494623656\n\n\n\n\n\n\n\n\n\nWith 4 layers, the leaf node seems quite reasonable.The model turns out to be quite trustable both on negative data. And favourite_count seems to be a remarkable metrics when grouping users."
  },
  {
    "objectID": "decision_tree.html#conclusion",
    "href": "decision_tree.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nThe model is not bad. It can correctly distinguish most samples and the accuracy is about 70%. However, since our target is to find users easily be affected by rumors. A false positive is more acceptable than a false negative, which means it is more acceptable to warn a person not easily be affected than fail to notify a person who may trust a rumor and pass it along! So we can consider to apply this model.\nDecision tree is unstable, so we can still use bagging(random forest), boosting(xgboost, GDBT, lightBGM, etc) to fit the data in the future."
  }
]