[
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "",
    "section": "",
    "text": "In this tab, a classification method called SVM(Supported Vector Machine) is used. This method ‚Äúdraw a bound‚Äù at the middle of the distance between the closest two samples.  \n\n(Example how svm functions)\n\n\nBeside, by appling ‚Äúkernel trick‚Äù, SVM can also draw non-linear lines to seperate points.  \n\n(Example how kernel svm functions)\n\n\nAdvantages of this method is: SVM performs reasonably well when the difference between classes is huge.\nWith SVM, we are going to solve the following application\n\n\nRumor Detection"
  },
  {
    "objectID": "svm.html#class-distribution",
    "href": "svm.html#class-distribution",
    "title": "",
    "section": "Class Distribution",
    "text": "Class Distribution\n\n\nCode\n# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\n#set the seed \nnp.random.seed(1)\n#load the data \ndf=pd.read_csv(\"../../data/01-modified-data/supervised_data.csv\")\ny=df[\"label\"]\n#plot the distribution of two classes\nsns.set_theme()\nplt.hist(y)\nplt.title(\"The distribution of the class\",fontsize=18)\nplt.xlabel(\"Class\",fontsize=16)\nplt.ylabel(\"Counts\",fontsize=16)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      text\n      location\n      friends_count\n      followers_count\n      screen_name\n      retweet_count\n      favorite_count\n      label\n      description\n    \n  \n  \n    \n      0\n      is that you tome hanks?...how about we keep am...\n      NaN\n      13006\n      31014\n      helen henning\n      763\n      4208\n      rumor\n      #ULTRA MAGA...#KAG...let's have some fun...sad...\n    \n    \n      1\n      After the dreadful hurricane in Florida @VP  K...\n      London\n      31163\n      33758\n      David Atherton\n      198\n      297\n      rumor\n      Libertarian, free markets, free speech. \"Selfi...\n    \n    \n      2\n      Heartbreaking! Iranian father who promised to ...\n      Nature\n      4441\n      9363\n      Shukri Hamk\n      9589\n      30927\n      rumor\n      -Survivor of #YazidiGenocide No.74 By #ISIS. -...\n    \n    \n      3\n      my dad just sent me this video from Naples Flo...\n      NaN\n      70\n      29\n      the worlds foremost authority\n      933\n      3681\n      rumor\n      i like to think i‚Äôm funny\n    \n    \n      4\n      Omg so all funerals due on the 19 th have been...\n      NaN\n      4795\n      3928\n      Carolyn Brown\n      3381\n      26141\n      rumor\n      Feminist, Cymraes and European! üè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åøüá™üá∫üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£...\n    \n  \n\n\n\n\n\n\n\nThis is an imbalanced data, the number of truth is bigger than the number of rumor. This is induced by two different ways of collecting data. Rumor samples are rather hard to get. In this model, we will try to use sample a equal number of truth from the rows where label is ‚Äútruth‚Äù."
  },
  {
    "objectID": "svm.html#baseline-model-for-comparsion",
    "href": "svm.html#baseline-model-for-comparsion",
    "title": "",
    "section": "Baseline Model for Comparsion",
    "text": "Baseline Model for Comparsion\n\n\nCode\n# transform the label \ny=y.str.replace(\"rumor\",\"1\")\ny=y.str.replace(\"truth\",\"0\")\ny=y.astype(\"int\")\n#set a baseline model which random predict label\ndef random_classifier(y_data):\n    ypred=[]\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data,ypred))\n\nrandom_classifier(y)\n\n\n-----RANDOM CLASSIFIER-----\naccuracy 0.4612476370510397\npercision, recall, fscore, (array([0.92828685, 0.03956835]), array([0.466     , 0.37931034]), array([0.62050599, 0.07166124]), array([500,  29], dtype=int64))\n\n\n What a baseline model here did is guess the class. And we can see that every metric is around 50%. So if a model perform than this baseline, than we can say it do make some sense."
  },
  {
    "objectID": "svm.html#feature-selection",
    "href": "svm.html#feature-selection",
    "title": "",
    "section": "Feature Selection",
    "text": "Feature Selection\n\n\nCode\n#sample a subset of negative samples\na=df[df[\"label\"]==\"truth\"].sample(100)\nb=df[df['label']==\"rumor\"]\ndf=pd.concat([a,b])\ndf.reset_index(drop=True, inplace=True)\ny=df['label']\nX=df[\"text\"]\ny=y.str.replace(\"rumor\",\"1\")\ny=y.str.replace(\"truth\",\"0\")\ny=y.astype(\"int\")\n#transform texts with countvectorizer\nvectorizer = CountVectorizer()\nmatrix = vectorizer.fit_transform(X)\nX = pd.DataFrame(matrix.toarray(),columns=vectorizer.get_feature_names_out())\n#split the data\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\n\nThis model try to use texts to classify, so our features are texts transformed by ‚ÄúCountVectorizer‚Äù"
  },
  {
    "objectID": "svm.html#model-tuning",
    "href": "svm.html#model-tuning",
    "title": "",
    "section": "Model tuning",
    "text": "Model tuning\n\n\nCode\n#find the best hyperparametres with GridSearchCV library\nparameter=[\n    {\"C\":[1,10,100,1000],\"kernel\":[\"linear\"]},\n    {\"C\":[1,10,100,1000],\"kernel\":[\"rbf\"],\"gamma\":[0.1,.2,.3,.4,.5,.6,.7,.8,.9]}\n]\ngrid_search = GridSearchCV(SVC(), param_grid=parameter, scoring=\"accuracy\",cv=10)\ngrid_search=grid_search.fit(X, y)\nprint(\"The best hyperparametres are:\",grid_search.best_params_)\ngrid_search\n\n\nThe best hyperparametres are: {'C': 1, 'kernel': 'linear'}\n\n\nGridSearchCV(cv=10, estimator=SVC(),\n             param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n                         {'C': [1, 10, 100, 1000],\n                          'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,\n                                    0.9],\n                          'kernel': ['rbf']}],\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=10, estimator=SVC(),\n             param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n                         {'C': [1, 10, 100, 1000],\n                          'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,\n                                    0.9],\n                          'kernel': ['rbf']}],\n             scoring='accuracy')estimator: SVCSVC()SVCSVC()\n\n\nIn this part, we use ‚ÄúGridSearchCV‚Äù function to help us find the beset parametres so that we don‚Äôt need to write codes and make some loops manually. The result shows that we should use linear kernel and should set C as 1."
  },
  {
    "objectID": "svm.html#final-results",
    "href": "svm.html#final-results",
    "title": "",
    "section": "Final Results",
    "text": "Final Results\n\n\nCode\n#write a function to report and plot the metrics and confusion matrix.\ndef confusion_plot(y_data,y_pred):\n    print(\n        \"ACCURACY: \"+str(accuracy_score(y_data,y_pred))+\"\\n\"+\n        \"NEGATIVE RECALL (Y=0): \"+str(recall_score(y_data,y_pred,pos_label=0))+\"\\n\"+\n        \"NEGATIVE PRECISION (Y=0): \"+str(precision_score(y_data,y_pred,pos_label=0))+\"\\n\"+\n        \"POSITIVE RECALL (Y=1): \"+str(recall_score(y_data,y_pred,pos_label=1))+\"\\n\"+\n        \"POSITIVE PRECISION (Y=1): \"+str(precision_score(y_data,y_pred,pos_label=1))+\"\\n\"\n    )\n    cf=confusion_matrix(y_data, y_pred)\n    # customize the anno\n    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n    group_counts = [\"{0:0.0f}\".format(value) for value in cf.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    #plot the heatmap\n    fig=sns.heatmap(cf, annot=labels, fmt=\"\", cmap='Blues')\n    plt.title(\"Confusion Matrix of Texts - Decision Tree\",fontsize=18)\n    fig.set_xticklabels([\"Easily affected\",\"Not easily affected\"],fontsize=13)\n    fig.set_yticklabels([\"Easily affected\",\"Not easily affected\"],fontsize=13)\n    fig.set_xlabel(\"Predicted Labels\",fontsize=14)\n    fig.set_ylabel(\"True Labels\",fontsize=14)\n    plt.show()\n#fit the model with the best hyperparametres\nclf=SVC(C=1,kernel=\"linear\")\nclf.fit(x_train,y_train)\nyp_test=clf.predict(x_test)\nconfusion_plot(y_test,yp_test)\n\n\nACCURACY: 0.8076923076923077\nNEGATIVE RECALL (Y=0): 0.95\nNEGATIVE PRECISION (Y=0): 0.8260869565217391\nPOSITIVE RECALL (Y=1): 0.3333333333333333\nPOSITIVE PRECISION (Y=1): 0.6666666666666666\n\n\n\n\n\n\nIt seems that even we have used the best hyperparametre, however, the effect of svm is still bad. We can not confidently make judgement basde on this model, since it can hardly distinguish positive samples."
  },
  {
    "objectID": "svm.html#conclusion",
    "href": "svm.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nIt turns out SVM is not suitable for our topic,performs much worse than naive bayes when facing the same task. The reason may be that we haven‚Äôt collected enought data.\nComparing to the naive bayes model, it seems naive bayes still outperform SVM on this task even though its a simple model."
  }
]