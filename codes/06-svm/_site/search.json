[
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "",
    "section": "",
    "text": "In this tab, a classification method called SVM(Supported Vector Machine) is used. This method “draw a bound” at the middle of the distance between the closest two samples.  \n\n(Example how svm functions)\n\n\nAdvantages of this method is: SVM performs reasonably well when the difference between classes is huge.\nWith SVM, we are going to solve the following application\n\n\nDistinguish rumors from truth"
  },
  {
    "objectID": "svm.html#class-distribution",
    "href": "svm.html#class-distribution",
    "title": "",
    "section": "Class Distribution",
    "text": "Class Distribution\n\n\nCode\n# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\ndf=pd.read_csv(\"../../data/01-modified-data/supervised_data.csv\")\ny=df[\"label\"]\nsns.set_theme()\nplt.hist(y)\nplt.title(\"The distribution of the class\",fontsize=18)\nplt.xlabel(\"Class\",fontsize=16)\nplt.ylabel(\"Counts\",fontsize=16)\n\n\nText(0, 0.5, 'Counts')\n\n\n\n\n\nThis is an imbalanced data, the number of truth is bigger than the number of rumor. This is induced by two different ways of collecting data. Rumor samples are rather hard to get. In this model, we will try to use sample a equal number of truth from the rows where label is “truth”."
  },
  {
    "objectID": "svm.html#baseline-model-for-comparsion",
    "href": "svm.html#baseline-model-for-comparsion",
    "title": "",
    "section": "Baseline Model for Comparsion",
    "text": "Baseline Model for Comparsion\n\n\nCode\ny=y.str.replace(\"rumor\",\"1\")\ny=y.str.replace(\"truth\",\"0\")\ny=y.astype(\"int\")\ndef random_classifier(y_data):\n    ypred=[]\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data,ypred))\n\nrandom_classifier(y)\n\n\n-----RANDOM CLASSIFIER-----\naccuracy 0.5236294896030246\npercision, recall, fscore, (array([0.95588235, 0.06614786]), array([0.52     , 0.5862069]), array([0.67357513, 0.11888112]), array([500,  29], dtype=int64))\n\n\n What a baseline model here did is guess the class. And we can see that every metric is around 50%. So if a model perform than this baseline, than we can say it do make some sense."
  },
  {
    "objectID": "svm.html#feature-selection",
    "href": "svm.html#feature-selection",
    "title": "",
    "section": "Feature Selection",
    "text": "Feature Selection\n\n\nCode\n#sample a subset of negative samples\na=df[df[\"label\"]==\"truth\"].sample(100)\nb=df[df['label']==\"rumor\"]\ndf=pd.concat([a,b])\ndf.reset_index(drop=True, inplace=True)\ny=df['label']\nX=df[\"text\"]\ny=y.str.replace(\"rumor\",\"1\")\ny=y.str.replace(\"truth\",\"0\")\ny=y.astype(\"int\")\n#transform texts\nvectorizer = CountVectorizer()\nmatrix = vectorizer.fit_transform(X)\nX = pd.DataFrame(matrix.toarray(),columns=vectorizer.get_feature_names_out())\n#split the data\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\n\nThis model try to use texts to classify, so our features are texts transformed by “CountVectorizer”"
  },
  {
    "objectID": "svm.html#model-tuning",
    "href": "svm.html#model-tuning",
    "title": "",
    "section": "Model tuning",
    "text": "Model tuning\n\n\nCode\ndef confusion_plot(y_data,y_pred):\n    print(\n        \"ACCURACY: \"+str(accuracy_score(y_data,y_pred))+\"\\n\"+\n        \"NEGATIVE RECALL (Y=0): \"+str(recall_score(y_data,y_pred,pos_label=0))+\"\\n\"+\n        \"NEGATIVE PRECISION (Y=0): \"+str(precision_score(y_data,y_pred,pos_label=0))+\"\\n\"+\n        \"POSITIVE RECALL (Y=1): \"+str(recall_score(y_data,y_pred,pos_label=1))+\"\\n\"+\n        \"POSITIVE PRECISION (Y=1): \"+str(precision_score(y_data,y_pred,pos_label=1))+\"\\n\"\n    )\n    cf=confusion_matrix(y_data, y_pred)\n    # customize the anno\n    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n    group_counts = [\"{0:0.0f}\".format(value) for value in cf.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    #plot the heatmap\n    fig=sns.heatmap(cf, annot=labels, fmt=\"\", cmap='Blues')\n    plt.title(\"Confusion Matrix of Texts - Decision Tree\",fontsize=18)\n    fig.set_xticklabels([\"Easily affected\",\"Not easily affected\"],fontsize=13)\n    fig.set_yticklabels([\"Easily affected\",\"Not easily affected\"],fontsize=13)\n    fig.set_xlabel(\"Predicted Labels\",fontsize=14)\n    fig.set_ylabel(\"True Labels\",fontsize=14)\n    plt.show()\nclf=SVC(C=.4,kernel=\"linear\")\nclf.fit(x_train,y_train)\nyp_test=clf.predict(x_test)\nconfusion_plot(y_test,yp_test)\nclf=SVC(C=.4,kernel=\"poly\")\nclf.fit(x_train,y_train)\nyp_test=clf.predict(x_test)\nconfusion_plot(y_test,yp_test)\nclf=SVC(C=.4,kernel=\"rbf\")\nclf.fit(x_train,y_train)\nyp_test=clf.predict(x_test)\nconfusion_plot(y_test,yp_test)\nclf=SVC(C=.4,kernel=\"sigmoid\")\nclf.fit(x_train,y_train)\nyp_test=clf.predict(x_test)\nconfusion_plot(y_test,yp_test)\n\n\nACCURACY: 0.8076923076923077\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 0.8\nPOSITIVE RECALL (Y=1): 0.16666666666666666\nPOSITIVE PRECISION (Y=1): 1.0\n\n\n\n\n\n\nACCURACY: 0.7692307692307693\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 0.7692307692307693\nPOSITIVE RECALL (Y=1): 0.0\nPOSITIVE PRECISION (Y=1): 0.0\n\n\n\nC:\\Users\\Yifan\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning:\n\nPrecision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n\n\n\n\n\n\nACCURACY: 0.7692307692307693\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 0.7692307692307693\nPOSITIVE RECALL (Y=1): 0.0\nPOSITIVE PRECISION (Y=1): 0.0\n\n\n\nC:\\Users\\Yifan\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning:\n\nPrecision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n\n\n\n\n\n\nC:\\Users\\Yifan\\anaconda3\\envs\\ANLY501\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning:\n\nPrecision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n\n\n\nACCURACY: 0.7692307692307693\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 0.7692307692307693\nPOSITIVE RECALL (Y=1): 0.0\nPOSITIVE PRECISION (Y=1): 0.0\n\n\n\n\n\n\n We have tried different kernel, however, whatever kernel we use, the result is totally useless ,much worse than the baseline!"
  },
  {
    "objectID": "svm.html#final-results",
    "href": "svm.html#final-results",
    "title": "",
    "section": "Final Results",
    "text": "Final Results\nIt seems that linear kernel do the best job. However, actually they are all the same bad. We can not confidently make judgement basde on this model."
  },
  {
    "objectID": "svm.html#conclusion",
    "href": "svm.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nIt turns out SVM is not suitable for our topic,performs much worse than naive bayes when facing the same task."
  }
]