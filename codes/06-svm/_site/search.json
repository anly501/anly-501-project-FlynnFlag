[
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "",
    "section": "",
    "text": "In this tab, a classification method called SVM(Supported Vector Machine) is used. This method “draw a bound” at the middle of the distance between the closest two samples.  \n\n(Example how svm functions)\n\n\nAdvantages of this method is: SVM performs reasonably well when the difference between classes is huge.\nWith SVM, we are going to solve the following application\n\n\nDistinguish rumors from truth"
  },
  {
    "objectID": "svm.html#class-distribution",
    "href": "svm.html#class-distribution",
    "title": "",
    "section": "Class Distribution",
    "text": "Class Distribution\n\n\nCode\n# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\ndf=pd.read_csv(\"../../data/01-modified-data/cleaned_followers.csv\")\nsns.set_theme()\nplt.hist(df.label.astype(\"string\"))\nplt.title(\"The distribution of the class\",fontsize=18)\nplt.xlabel(\"Class\",fontsize=16)\nplt.ylabel(\"Counts\",fontsize=16)\n\n\nText(0, 0.5, 'Counts')\n\n\n\n\n\nThe count of two classes are the same, 350 for each. This is dsigned when gathering the data(“details of followers”). With this kind of data, we can avoid problems brought from imbalanced data sets."
  },
  {
    "objectID": "svm.html#baseline-model-for-comparsion",
    "href": "svm.html#baseline-model-for-comparsion",
    "title": "",
    "section": "Baseline Model for Comparsion",
    "text": "Baseline Model for Comparsion\n\n\nCode\ndef random_classifier(y_data):\n    ypred=[]\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data,ypred))\n\nrandom_classifier(df.label)\n\n\n-----RANDOM CLASSIFIER-----\naccuracy 0.49857142857142855\npercision, recall, fscore, (array([0.49864499, 0.49848943]), array([0.52571429, 0.47142857]), array([0.51182197, 0.4845815 ]), array([350, 350], dtype=int64))\n\n\n What a baseline model here did is guess the class. And we can see that every metric is around 50%. So if a model perform than this baseline, than we can say it do make some sense."
  },
  {
    "objectID": "svm.html#feature-selection",
    "href": "svm.html#feature-selection",
    "title": "",
    "section": "Feature Selection",
    "text": "Feature Selection\n\n\nCode\n# id reflect the time account exits,we normalized it \ndf[\"user_id\"]=(df.user_id-df.user_id.mean())/df.user_id.std() \n#drop the feature we won't consider about\ndf.drop(columns=[\"screen_name\"],axis=1,inplace=True)\n#\nX=df.drop(columns=[\"label\"],axis=1)\ny=df[\"label\"]\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\n\n\n\n\n\n\n\n\nFeature\nMeaning\n\n\n\n\nuser_id\nthe id of users\n\n\nfollowers_count\nthe number of followers this account currently has\n\n\nfriends_count\nthe number of users this account is following\n\n\nlisted_count\nthe number of public lists that this user is a member of\n\n\nfavourites_count\nthe number of Tweets this user has liked in the account’s lifetime\n\n\ntweet_num\nthe number of Tweets (including retweets) issued by the user\n\n\nprotected\nwhether user has chosen to protect their Tweets\n\n\nverified\nwhether user has a verified account\n\n\n\n\n(The names and meanings of features)\n\n8 features was selected to train the model. These features are all attributes that are allowed to get and reflect the character of accounts. With these attributes, we have the biggest possibility to find the differences. Note that “user_id” is also selected because it reflect how long an account was created. The bigger an user id is, the newer the account is."
  },
  {
    "objectID": "svm.html#model-tuning",
    "href": "svm.html#model-tuning",
    "title": "",
    "section": "Model tuning",
    "text": "Model tuning\n\n\nCode\n# try different numbers of layers to find the best one\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train,y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0),recall_score(y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label=0),recall_score(y_train, yp_train,pos_label=1)])\ntest_results=np.array(test_results)\ntrain_results=np.array(train_results)\n#generate plots of the performance of different layers \ndef metric_plot(ylabel,layer,yptrain,yptest):\n    fig=plt.figure()\n    plt.plot(layer,yptrain,'o-',color=\"b\")\n    plt.plot(layer,yptest,'o-',color=\"r\")\n    plt.ylabel(ylabel+\" Training (blue) and Test (red)\",fontsize=16)\n    plt.xlabel(\"Number of layers in decision tree(max_depth)\",fontsize=16)\nmetric_plot(\"ACCURACY(Y=0)\",test_results[:,0],train_results[:,1],test_results[:,1])\nmetric_plot(\"RECALL(Y=0)\",test_results[:,0],train_results[:,2],test_results[:,2])\nmetric_plot(\"RECALL(Y=1)\",test_results[:,0],train_results[:,3],test_results[:,3])\n\n\n\n\n\n\n\n\n\n\n\n To find the most suitable number of layers, several plots was produced. We finally find that we should set max_depth as 3."
  },
  {
    "objectID": "svm.html#final-results",
    "href": "svm.html#final-results",
    "title": "",
    "section": "Final Results",
    "text": "Final Results\n\n\nCode\nmodel = tree.DecisionTreeClassifier(max_depth=3)\nmodel = model.fit(x_train,y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n#write a function to visualize the confusion matrix\ndef confusion_plot(y_data,y_pred):\n    print(\n        \"ACCURACY: \"+str(accuracy_score(y_data,y_pred))+\"\\n\"+\n        \"NEGATIVE RECALL (Y=0): \"+str(recall_score(y_data,y_pred,pos_label=0))+\"\\n\"+\n        \"NEGATIVE PRECISION (Y=0): \"+str(precision_score(y_data,y_pred,pos_label=0))+\"\\n\"+\n        \"POSITIVE RECALL (Y=1): \"+str(recall_score(y_data,y_pred,pos_label=1))+\"\\n\"+\n        \"POSITIVE PRECISION (Y=1): \"+str(precision_score(y_data,y_pred,pos_label=1))+\"\\n\"\n    )\n    cf=confusion_matrix(y_data, y_pred)\n    # customize the anno\n    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n    group_counts = [\"{0:0.0f}\".format(value) for value in cf.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    #plot the heatmap\n    fig=sns.heatmap(cf, annot=labels, fmt=\"\", cmap='Blues')\n    plt.title(\"Confusion Matrix of Texts - Decision Tree\",fontsize=18)\n    fig.set_xticklabels([\"Easily affected\",\"Not easily affected\"],fontsize=13)\n    fig.set_yticklabels([\"Easily affected\",\"Not easily affected\"],fontsize=13)\n    fig.set_xlabel(\"Predicted Labels\",fontsize=14)\n    fig.set_ylabel(\"True Labels\",fontsize=14)\n    plt.show()\nconfusion_plot(y_test,yp_test)\n\n#write a function to visualize the tree\ndef plot_tree(model,X,Y):\n    fig = plt.figure(figsize=(10,8))\n    tree_vis= tree.plot_tree(model, feature_names=X.columns,class_names=Y.name,filled=True)\nplot_tree(model,x_test,y_test)\n\n\nACCURACY: 0.5928571428571429\nNEGATIVE RECALL (Y=0): 0.39344262295081966\nNEGATIVE PRECISION (Y=0): 0.5454545454545454\nPOSITIVE RECALL (Y=1): 0.7468354430379747\nPOSITIVE PRECISION (Y=1): 0.6145833333333334"
  },
  {
    "objectID": "svm.html#conclusion",
    "href": "svm.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nThe model is not bad. It can correctly distinguish most samples and the accuracy is about 70%. However, since our target is to find users easily be affected by rumors. A false positive is more acceptable than a false negative, which means it is more acceptable to warn a person not easily be affected than fail to notify a person who may trust a rumor and pass it along! So we can consider to apply this model."
  }
]