---
jupyter: python3

---


## Method 

In this tab, a classification method called ***SVM(Supported Vector Machine)*** is used. This method "draw a bound" at the middle of the distance between the closest two samples. 
<br></br>
![](svm.jpeg)
<center>*(Example how svm functions)*</center>
<br></br>

Advantages of this method is: SVM performs reasonably well when the difference between classes is huge.

With SVM, we are going to solve the following application

---

<center>**Distinguish rumors from truth**</center>

--- 

<br></br>

## Class Distribution
```{python}
# import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
df=pd.read_csv("../../data/01-modified-data/supervised_data.csv")
y=df["label"]
sns.set_theme()
plt.hist(y)
plt.title("The distribution of the class",fontsize=18)
plt.xlabel("Class",fontsize=16)
plt.ylabel("Counts",fontsize=16)
```

This is an imbalanced data, the number of truth is bigger than the number of rumor. This is induced by two different ways of collecting data. Rumor samples are rather hard to get. 
In this model, we will try to use sample a equal number of truth from the rows where label is "truth".
<br></br>


## Baseline Model for Comparsion
```{python}
y=y.str.replace("rumor","1")
y=y.str.replace("truth","0")
y=y.astype("int")
def random_classifier(y_data):
    ypred=[]
    max_label=np.max(y_data); #print(max_label)
    for i in range(0,len(y_data)):
        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))
    print("-----RANDOM CLASSIFIER-----")
    print("accuracy",accuracy_score(y_data, ypred))
    print("percision, recall, fscore,",precision_recall_fscore_support(y_data,ypred))

random_classifier(y)
```
<br></br>
What a baseline model here did is guess the class. And we can see that every metric is around 50%. So if a model perform than this baseline, than we can say it do make some sense.

## Feature Selection
```{python}
#sample a subset of negative samples
a=df[df["label"]=="truth"].sample(100)
b=df[df['label']=="rumor"]
df=pd.concat([a,b])
df.reset_index(drop=True, inplace=True)
y=df['label']
X=df["text"]
y=y.str.replace("rumor","1")
y=y.str.replace("truth","0")
y=y.astype("int")
#transform texts
vectorizer = CountVectorizer()
matrix = vectorizer.fit_transform(X)
X = pd.DataFrame(matrix.toarray(),columns=vectorizer.get_feature_names_out())
#split the data
x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2)
```

This model try to use texts to classify, so our features are texts transformed by "CountVectorizer"

<br></br>

## Model tuning
```{python}
def confusion_plot(y_data,y_pred):
    print(
        "ACCURACY: "+str(accuracy_score(y_data,y_pred))+"\n"+
        "NEGATIVE RECALL (Y=0): "+str(recall_score(y_data,y_pred,pos_label=0))+"\n"+
        "NEGATIVE PRECISION (Y=0): "+str(precision_score(y_data,y_pred,pos_label=0))+"\n"+
        "POSITIVE RECALL (Y=1): "+str(recall_score(y_data,y_pred,pos_label=1))+"\n"+
        "POSITIVE PRECISION (Y=1): "+str(precision_score(y_data,y_pred,pos_label=1))+"\n"
    )
    cf=confusion_matrix(y_data, y_pred)
    # customize the anno
    group_names = ["True Neg","False Pos","False Neg","True Pos"]
    group_counts = ["{0:0.0f}".format(value) for value in cf.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in cf.flatten()/np.sum(cf)]
    labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    #plot the heatmap
    fig=sns.heatmap(cf, annot=labels, fmt="", cmap='Blues')
    plt.title("Confusion Matrix of Texts - Decision Tree",fontsize=18)
    fig.set_xticklabels(["Easily affected","Not easily affected"],fontsize=13)
    fig.set_yticklabels(["Easily affected","Not easily affected"],fontsize=13)
    fig.set_xlabel("Predicted Labels",fontsize=14)
    fig.set_ylabel("True Labels",fontsize=14)
    plt.show()
clf=SVC(C=.4,kernel="linear")
clf.fit(x_train,y_train)
yp_test=clf.predict(x_test)
confusion_plot(y_test,yp_test)
clf=SVC(C=.4,kernel="poly")
clf.fit(x_train,y_train)
yp_test=clf.predict(x_test)
confusion_plot(y_test,yp_test)
clf=SVC(C=.4,kernel="rbf")
clf.fit(x_train,y_train)
yp_test=clf.predict(x_test)
confusion_plot(y_test,yp_test)
clf=SVC(C=.4,kernel="sigmoid")
clf.fit(x_train,y_train)
yp_test=clf.predict(x_test)
confusion_plot(y_test,yp_test)

```
<br></br>
We have tried different kernel, however, whatever kernel we use, the result is totally useless ,much worse than the baseline! 


## Final Results
It seems that linear kernel do the best job. However, actually they are all the same bad. We can not confidently make judgement basde on this model.
<br></br>

## Conclusion

It turns out SVM is not suitable for our topic,performs much worse than naive bayes when facing the same task.