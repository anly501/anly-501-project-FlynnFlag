[
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "",
    "section": "",
    "text": "In this tab, we try to use clustering to group people who are more likely to be affected by rumors and people who are not likely to be. The data set used is “cleaned_followers.csv”.\nSince we try to use clustering, we drop all categorical columns. Additionally, this is a unsupervised data, so we drop the label column. Besides, regularization will lead to a better result, so we also regularize the features. The user_id is taken into consideration because it reflect when the account is created."
  },
  {
    "objectID": "clustering.html#theory",
    "href": "clustering.html#theory",
    "title": "",
    "section": "Theory",
    "text": "Theory\nThree clustering methods will be used in the following content, namely,KMEAN,DBSAN and Hierarchical Clustering. These tree methods are all based on the similarity(by calculating euclidean distance) of the instance.\nKmeans clusters the instances by continuously searching K centers.\nDBSAN groups the data points which the extent of similarity is below a certain level.\nHierarchical Clustering sequentially combine the most similar instances/groups. The similarity in a combination should below a certain level."
  },
  {
    "objectID": "clustering.html#method",
    "href": "clustering.html#method",
    "title": "",
    "section": "Method",
    "text": "Method\nThis part will show the workflow of training the optimal models with three methods we’ve talked about. \n\nData Selection\nThe features we use is shown below. As mentioned before, I remove the labels and categorical columns as well as conduct standardlization.\n\n\nCode\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n# read the data\ndf=pd.read_csv(\"../../data/01-modified-data/cleaned_followers.csv\")\n# remove the labels and categorical columns\ndf=df.drop([\"screen_name\",\"protected\",\"verfied\",\"label\"],axis=1)\n# conduct standarization\ncol=df.columns\ndf =  pd.DataFrame(StandardScaler().fit_transform(df),columns=col)\n#drop the outliers\ndf=df[df[\"friends_count\"]<5]\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      user_id\n      followers_count\n      friends_count\n      listed_count\n      favourites_count\n      tweet_num\n    \n  \n  \n    \n      0\n      -1.462589\n      -0.114256\n      -0.133278\n      -0.112242\n      -0.260739\n      -0.260023\n    \n    \n      1\n      0.761603\n      -0.109150\n      0.330401\n      -0.112242\n      -0.289027\n      -0.269354\n    \n    \n      2\n      0.875774\n      -0.116809\n      -0.159058\n      -0.112242\n      -0.287897\n      -0.269836\n    \n    \n      3\n      -1.462589\n      -0.112081\n      -0.156894\n      -0.102315\n      0.122106\n      -0.089817\n    \n    \n      4\n      -1.462589\n      -0.083147\n      0.029875\n      0.076364\n      0.574516\n      1.351783\n    \n  \n\n\n\n\n\n\n\nHyperparameter Tuning\nFor KMEAN, the hyperparameter tuning is based on the elbow method and the Silhouette methods. Normally, the K is picked when there is a turning point on the curve.\n\n\nCode\nimport warnings\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nwarnings.filterwarnings('ignore')\nWCSS=[]\nfor i in range(1,11):\n    kmeanModel=KMeans(n_clusters=i,init=\"k-means++\",random_state=42)\n    kmeanModel.fit(df)\n    WCSS.append(kmeanModel.inertia_)\nsns.set_theme()\nplt.plot(range(1,11),WCSS)\nplt.title(\"The elbow method for optimal K\",fontsize=20)\nplt.xlabel(\"The value of K\",fontsize=16)\nplt.ylabel(\"Within cluster sum of squares(WCSS)\",fontsize=16)\n\n\nText(0, 0.5, 'Within cluster sum of squares(WCSS)')\n\n\n\n\n\nThe elbow curve tells us that the best K value is 7\n\n\nCode\nimport sklearn.cluster\n\n# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) \n# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\ndef maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X) \n\n    # LOOP OVER HYPER-PARAM\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.5*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue \n\n        if(i_print): print(param,sil_scores[-1])\n        \n        if(sil_scores[-1]>sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")  \n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\nopt_labels=maximize_silhouette(df,algo=\"kmeans\",nmax=15, i_plot=True)\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\nWhen applying silhouette method, the best number of groups is 2.\nSince originally I need to divide the data set into two groups. To associate with my target, I choose 2 as my hyperparameter\nFor DBSN, we use nearestneighbors to find the optimal value of epsilon.\n\n\nCode\n# we use nearestneighbors for calculating distance between points\nfrom sklearn.neighbors import NearestNeighbors\nimport numpy as np\n# calculating distances\nneigh=NearestNeighbors(n_neighbors=2)\ndistance=neigh.fit(df)\n# indices and distance values\ndistances,indice=distance.kneighbors(df)\n# Now sorting the distance increasing order\nsort_distances=np.sort(distances,axis=0)\n# sorted distances\nsorted_distances=sort_distances[:,1]\n# plot between distance vs epsilon\nplt.plot(sorted_distances)\nplt.xlabel('Distance')\nplt.ylabel('Epsilon')\nplt.show()\n\n\n\n\n\nWhen we observe the graph, at epsilon is equal to 2 sharp rises in the graph so we choose epsilon(radius) as 2\nFor Hierarchy Clustering, since the originial data set is binarily seperated, we set k as 2, and use “ward”, which is robost to noise and outliers.\n\n\n\nFinal Results\nKMEAN\n\n\nCode\nkmeanModel=KMeans(n_clusters=2,init=\"k-means++\",random_state=42)\nkmeanModel.fit(df)\nkmean_label=kmeanModel.labels_\n\n\nDBSAN\n\n\nCode\nfrom sklearn.cluster import DBSCAN\n\nmodel=DBSCAN(eps=2.5,min_samples=2).fit(df)\nlabels_DB=model.labels_\n\n\nHierarchy Clustering:\n\n\nCode\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\n#call the model\nclustering_model=AgglomerativeClustering(n_clusters = 2, affinity = \"euclidean\", linkage = \"ward\")\nclustering_model.fit(df)\n# Predicting clusters\nHierarchy_label=clustering_model.labels_"
  },
  {
    "objectID": "clustering.html#results",
    "href": "clustering.html#results",
    "title": "",
    "section": "Results",
    "text": "Results\nKMEANS:\n\n\nCode\ndf[\"kmean\"]=kmean_label\na=sns.scatterplot(x=\"friends_count\",y=\"tweet_num\",hue=\"kmean\",data=df)\na.set_title(\"Kmean\",fontsize=20)\n\n\nText(0.5, 1.0, 'Kmean')\n\n\n\n\n\nDBSAN:\n\n\nCode\ndf[\"DBSAN\"]=labels_DB\na=sns.scatterplot(x=\"friends_count\",y=\"tweet_num\",hue=\"DBSAN\",data=df)\na.set_title(\"DBSAN\",fontsize=20)\n\n\nText(0.5, 1.0, 'DBSAN')\n\n\n\n\n\nHierarchy_clustering:\n\n\nCode\ndf[\"hierarchy\"]=Hierarchy_label\na=sns.scatterplot(x=\"friends_count\",y=\"tweet_num\",hue=\"hierarchy\",data=df)\na.set_title(\"Hierarchy Clustering\",fontsize=20)\n\n\nText(0.5, 1.0, 'Hierarchy Clustering')\n\n\n\n\n\nAccording to the results, KMEAN and hierarchy clustering returns a similar result. They sepearte the accounts which are more active with the others.For DBSAN, the best number of group calculated based on mathematical method is not what we want(2 groups).\nThe reason may be that just based on the information about account, the meaning of group is not exactly what we target at. Tjat may shift to something like “whether they are more likely to tweet”.\nUnfortunately, clustering seems to fail to provide what we want."
  },
  {
    "objectID": "clustering.html#conclusion",
    "href": "clustering.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tab, we have used three separate clustering model with the best hyperparameters to conduct unsupervised learning.\nClustering is not a suitable candidate to meet our demand. This is mainly because the result is not what we want.\nTo be specific, the result clustering gives may refers to the active status. This give us a hint that active status may have some connections to the groups we want, but can not directly tell us whether they are easily be affected by rumors"
  },
  {
    "objectID": "clustering.html#reference",
    "href": "clustering.html#reference",
    "title": "",
    "section": "Reference",
    "text": "Reference\n[1]Shaik, J. (2020, September 18). Practical implementation of K-means, hierarchical, and DBSCAN clustering on dataset with… Medium. Retrieved November 13, 2022, from https://medium.com/analytics-vidhya/practical-implementation-of-k-means-hierarchical-and-dbscan-clustering-on-dataset-with-bd7f3d13ef7f\n[2]Brus, P. (2021, July 29). Clustering: How to find hyperparameters using inertia. Medium. Retrieved November 13, 2022, from https://medium.com/towards-data-science/clustering-how-to-find-hyperparameters-using-inertia-b0343c6fe819"
  }
]